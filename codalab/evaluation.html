<h1>MetaDL Evaluation</h1>
<p>For both Feedback Phase and Public Phase, the performance of a meta-learning algorithm is measured through the evaluation of 600 episodes at <strong>meta-test</strong> time. The participant needs to implement a <code>MyMetaLearner</code> class that can meta-fit a meta-train set and produce a <code>Learner</code> object, which in turn can fit any support set (a.k.a training set), generated from a meta-test set, and produce a <code>Predictor</code>. The accuracy of these predictors on each query set (or test set) is then averaged to produce a final score. In Feedback Phase, this score is used to form a leaderboard. In Final Phase, this score is used as the criterion for deciding winners (and a leadberboard will also be released). One important aspect of the challenge is that submissions must produce a <code>Learner</code> within 2 hours for each meta-dataset (10 hours in total for all 5 meta-datasets). Each submission has access to 4 GPU <strong>Tesla M60&nbsp;</strong>for this amount of time.</p>
<h3>Episodes at meta-test time</h3>
<p>For each meta-dataset, we use the 5-way 1-shot few-shot learning setting:</p>
<p><strong>Support set:</strong> 5 classes and 1 example per class (labelled examples) <br /><strong>Query set:</strong> 5 classes and a <strong>varying number</strong> of examples per class (unlabelled examples)</p>
<p>After we compute the accuracy for all 5 meta-datasets, the&nbsp;<strong>overall ranking</strong>&nbsp;is used as the final score for evaluation and will be used in the learderboard. It is computed by averaging the ranks (among all participants) of accuracies obtained on the 5 meta-datasets.</p>